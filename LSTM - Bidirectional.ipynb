{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import emoji\n",
    "import re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1029536486021099522</td>\n",
       "      <td>@Gazo1a Nossa! Muito obrigada :)</td>\n",
       "      <td>Wed Aug 15 01:13:20 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1029536496368406528</td>\n",
       "      <td>@BerzGamer vai pa puta que te pariu :)</td>\n",
       "      <td>Wed Aug 15 01:13:23 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1029536531655131137</td>\n",
       "      <td>QUER MAIS DESCONTOS? (14/08) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê üåê Confira n...</td>\n",
       "      <td>Wed Aug 15 01:13:31 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1029536560117678081</td>\n",
       "      <td>EU VOU PEGAR VCS, ME AJUDEM GALERA, PELO AMOR ...</td>\n",
       "      <td>Wed Aug 15 01:13:38 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1029536605852377088</td>\n",
       "      <td>Est√°vamos em casa do Z√© e eu estava a morrer d...</td>\n",
       "      <td>Wed Aug 15 01:13:49 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         tweet_text  \\\n",
       "0  1029536486021099522                   @Gazo1a Nossa! Muito obrigada :)   \n",
       "1  1029536496368406528             @BerzGamer vai pa puta que te pariu :)   \n",
       "2  1029536531655131137  QUER MAIS DESCONTOS? (14/08) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê üåê Confira n...   \n",
       "3  1029536560117678081  EU VOU PEGAR VCS, ME AJUDEM GALERA, PELO AMOR ...   \n",
       "4  1029536605852377088  Est√°vamos em casa do Z√© e eu estava a morrer d...   \n",
       "\n",
       "                       tweet_date  sentiment query_used  \n",
       "0  Wed Aug 15 01:13:20 +0000 2018          1         :)  \n",
       "1  Wed Aug 15 01:13:23 +0000 2018          1         :)  \n",
       "2  Wed Aug 15 01:13:31 +0000 2018          1         :)  \n",
       "3  Wed Aug 15 01:13:38 +0000 2018          1         :)  \n",
       "4  Wed Aug 15 01:13:49 +0000 2018          1         :)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "df_test = pd.read_csv(\"Test.csv\", sep=';', header=0)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                             obrigada\n",
       "1                                    vai pa puta pariu\n",
       "2    quer descontos 1408 confira link compartilhe e...\n",
       "3              vou pegar vcs ajudem galera amor butera\n",
       "4             casa ze morrer sono chego casa fico sono\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading test data\n",
    "test_data = df_test['tweet_text']\n",
    "\n",
    "# From upper case to lower case    \n",
    "test_data = test_data.str.lower() \n",
    "print(test_data.shape)\n",
    "\n",
    "# Removing urls, @users content , #hashtag\n",
    "test_data = test_data.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True).replace(r'@\\S+', '', regex=True).replace(r'#\\S+', '', regex=True)\n",
    "print(test_data.shape)\n",
    "\n",
    "# Removing punctuation & emojis\n",
    "test_data = test_data.str.replace('[^\\w\\s]','') \n",
    "print(test_data.shape)\n",
    "\n",
    "# Removing stopwords\n",
    "for twitt in range(0,test_data.shape[0]):\n",
    "    clean_twitt = []\n",
    "    test_data[twitt] = word_tokenize(test_data[twitt])\n",
    "    for w in test_data[twitt]:\n",
    "        if w not in stop_words: \n",
    "            clean_twitt.append(w)\n",
    "    test_data[twitt] = clean_twitt\n",
    "    test_data[twitt] = ' '.join(test_data[twitt])\n",
    "    # Removing accents\n",
    "    test_data[twitt] = unidecode.unidecode(test_data[twitt])\n",
    "    # Removing single characters\n",
    "    test_data[twitt] =  re.sub(r\"\\b[a-zA-Z]\\b\", \"\", test_data[twitt])\n",
    "    \n",
    "# Remove numbers ??\n",
    "\n",
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_process_data = df_test['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Un-clean data:\n",
      "@Gazo1a Nossa! Muito obrigada :)\n",
      "0 Clean data:\n",
      "obrigada\n",
      "-----------\n",
      "1  Un-clean data:\n",
      "@BerzGamer vai pa puta que te pariu :)\n",
      "1 Clean data:\n",
      "vai pa puta pariu\n",
      "-----------\n",
      "2  Un-clean data:\n",
      "QUER MAIS DESCONTOS? (14/08) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê üåê Confira no link ‚ûî https://t.co/jhh0Ttlpq3 | ¬ª Compartilhe! ¬´ N√£o encontrou a oferta ou cupom que procurava? N√£o se preocupe ;) criamos uma pagina com mais ofertas e cupons atualizada diariamente :) https://t.co/IgeeD0WmPf\n",
      "2 Clean data:\n",
      "quer descontos 1408 confira link compartilhe encontrou oferta cupom procurava preocupe criamos pagina ofertas cupons atualizada diariamente\n",
      "-----------\n",
      "3  Un-clean data:\n",
      "EU VOU PEGAR VCS, ME AJUDEM GALERA, PELO AMOR DE BUTERA. :) #MMVAs #iHeartRadioMMVAs #FFSingleNoTears https://t.co/ghZMLqRFiV\n",
      "3 Clean data:\n",
      "vou pegar vcs ajudem galera amor butera\n",
      "-----------\n",
      "4  Un-clean data:\n",
      "Est√°vamos em casa do Z√© e eu estava a morrer de sono, chego a casa e fico sem sono :))))\n",
      "4 Clean data:\n",
      "casa ze morrer sono chego casa fico sono\n",
      "-----------\n",
      "5  Un-clean data:\n",
      "Precisa :) https://t.co/DcLGDHbjT0\n",
      "5 Clean data:\n",
      "precisa\n",
      "-----------\n",
      "6  Un-clean data:\n",
      "@Jeniabreu07 mas por acaso adoro beijos e abra√ßos Hahahahaha √© bue fofiii ahahaha houve uma altura q n agora gosto :)\n",
      "6 Clean data:\n",
      "acaso adoro beijos abracos hahahahaha bue fofiii ahahaha altura   agora gosto\n",
      "-----------\n",
      "7  Un-clean data:\n",
      "Solteiro s sozinho sempre :)\n",
      "7 Clean data:\n",
      "solteiro  sozinho sempre\n",
      "-----------\n",
      "8  Un-clean data:\n",
      "Lindezas Cortador kit M√°rio cole√ß√£o Bia Cravol j√° esta cadastrado no site, quem estava esperando j√° pode ir adquirir o seu e arrasar nas encomendas, afinal √© um dos temas do momento :)... https://t.co/7eWQr2TnEX\n",
      "8 Clean data:\n",
      "lindezas cortador kit mario colecao bia cravol cadastrado site esperando pode ir adquirir arrasar encomendas afinal temas momento\n",
      "-----------\n",
      "9  Un-clean data:\n",
      "@otphurts Claro que sim :))\n",
      "9 Clean data:\n",
      "claro sim\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(i,\" Un-clean data:\")\n",
    "    print(not_process_data[i])\n",
    "    print(i, \"Clean data:\")\n",
    "    print(test_data[i])\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
